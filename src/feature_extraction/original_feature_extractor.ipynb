{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-31 23:03:25.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_folder\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed 0 file!\u001b[0m\n",
      "\u001b[32m2024-05-31 23:04:38.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_folder\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed 100 file!\u001b[0m\n",
      "\u001b[32m2024-05-31 23:05:32.079\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfeature_engineering_for_file\u001b[0m:\u001b[36m71\u001b[0m - \u001b[31m\u001b[1mError processing file D:/data_analysis/speech_emotion_recognition/data/EnglishDataset/train_test_splited_data/cleaned_with_pad_or_trim/\\train\\03-01-05-01-02-02-07.wav: CPUDispatcher(<function _viterbi at 0x000002D409861260>) returned a result with an exception set\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "import os\n",
    "import librosa\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from modules.mfcc_extractor import MfccExtractor\n",
    "from modules.pitch_extractor import PitchExtractor\n",
    "from modules.rms_extractor import RmsExtractor\n",
    "from modules.zcr_extractor import ZcrExtractor\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, n_mfcc=20):\n",
    "        self.n_mfcc = n_mfcc\n",
    "\n",
    "    def extract_original_mfcc(self, signal, sample_rate, frame_size, hop_size):\n",
    "        mfcc_extractor = MfccExtractor(signal, sample_rate, self.n_mfcc, frame_size, hop_size)\n",
    "        mfcc_matrix = mfcc_extractor.compute_mfccs()\n",
    "        mfcc_vectors = []\n",
    "        for row in range(mfcc_matrix.shape[0]):\n",
    "            mfcc_vector = mfcc_matrix[row, :]\n",
    "            mfcc_vectors.append(mfcc_vector)\n",
    "\n",
    "        return mfcc_vectors\n",
    "    \n",
    "    def extract_pitch(self, signal, sample_rate, frame_size, hop_size):\n",
    "        pitch_extractor = PitchExtractor(signal, sample_rate, frame_size, hop_size)\n",
    "        return pitch_extractor.compute_pitch()\n",
    "    \n",
    "    def extract_original_zcr(self, signal, frame_size, hop_size):\n",
    "        zcr_extractor = ZcrExtractor(signal, frame_size, hop_size)\n",
    "        zcr = zcr_extractor.compute_zcr()\n",
    "        return zcr\n",
    "    \n",
    "    def extract_original_rms(self, signal, frame_size, hop_size):\n",
    "        rms_extractor = RmsExtractor(signal, frame_size, hop_size)\n",
    "        return rms_extractor.compute_rms()\n",
    "    \n",
    "    def pad_or_trim(self, zcr, target_length):\n",
    "      if len(zcr) < target_length:\n",
    "          return np.pad(zcr, (0, target_length - len(zcr)), 'constant')\n",
    "      else:\n",
    "          return zcr[:target_length]\n",
    "    \n",
    "    def calculate_number_of_frames(self, audio_length, sample_rate, frame_length, hop_length):\n",
    "      \"\"\"\n",
    "      Tính số lượng khung dựa vào độ dài tín hiệu âm thanh, tần số lấy mẫu, frame length, và hop length.\n",
    "      \"\"\"\n",
    "      # Tính độ dài tín hiệu âm thanh bằng số mẫu\n",
    "      signal_length = int(audio_length * sample_rate)\n",
    "      \n",
    "      # Tính số lượng khung\n",
    "      num_frames = 1 + (signal_length - frame_length) // hop_length\n",
    "      \n",
    "      return num_frames\n",
    "\n",
    "    def feature_engineering_for_file(self, audio_file, frame_size=2048, hop_size=512):\n",
    "        try:\n",
    "            signal, sample_rate = librosa.load(audio_file, sr=None)\n",
    "            if len(signal) >= frame_size:\n",
    "                zcr = self.extract_original_zcr(signal, frame_size, hop_size)\n",
    "                rms = self.extract_original_rms(signal, frame_size, hop_size)\n",
    "                pitch = self.extract_pitch(signal, sample_rate, frame_size, hop_size)\n",
    "                mfccs = self.extract_original_mfcc(signal, sample_rate, frame_size, hop_size)\n",
    "                \n",
    "                stacked_mfcc = np.hstack(mfccs)\n",
    "\n",
    "                combined_features = np.hstack((zcr, rms, pitch, stacked_mfcc))\n",
    "                return combined_features\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing file {audio_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def process_folder(self, csv_file):\n",
    "        file_and_label_df = pd.read_csv(csv_file)\n",
    "        feature_dataframes = pd.DataFrame()  # DataFrame to store features from all audio files\n",
    "\n",
    "        for index, row in file_and_label_df.iterrows():\n",
    "            file_path = row['cleaned_file_path']\n",
    "            label = row['label']\n",
    "            features = self.feature_engineering_for_file(file_path)\n",
    "\n",
    "            if index % 100 == 0:\n",
    "                logger.info(f'Processed {index} file!')\n",
    "                feature_dataframes.to_csv('_train_feature_data.csv', index=False)\n",
    "\n",
    "            if features is not None:\n",
    "                features_dict = {\n",
    "                    'file_path': file_path,\n",
    "                    'label': label\n",
    "                }\n",
    "                for i, feature in enumerate(features):\n",
    "                    features_dict[f'feature_{i+1}'] = feature  # Assign feature values to corresponding columns in DataFrame\n",
    "                feature_dataframe = pd.DataFrame([features_dict])  # Convert the dictionary to a DataFrame\n",
    "                feature_dataframes = pd.concat([feature_dataframes, feature_dataframe], ignore_index=True)  # Append the new row to the aggregated DataFrame\n",
    "\n",
    "        # Save the aggregated DataFrame to a CSV file\n",
    "        feature_dataframes.to_csv('_train_feature_data.csv', index=False)\n",
    "    \n",
    "feature_extractor = FeatureExtractor()\n",
    "feature_extractor.process_folder(r\"D:\\data_analysis\\speech_emotion_recognition\\data\\EnglishDataset\\train_test_splited_data\\cleaned_with_pad_or_trim\\train_file_paths_with_labels.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
