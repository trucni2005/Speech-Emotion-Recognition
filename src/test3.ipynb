{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu đã được chia thành công!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Đường dẫn tới thư mục gốc chứa dữ liệu\n",
    "original_dataset_dir = 'D:/data_analysis/speech_emotion_recognition/data/EnglishDataset/images'\n",
    "\n",
    "# Đường dẫn tới các thư mục mới\n",
    "base_dir = 'D:/data_analysis/speech_emotion_recognition/data/EnglishDataset/split_data'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Tạo các thư mục nếu chưa tồn tại\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Lấy danh sách các lớp từ thư mục gốc\n",
    "classes = [d for d in os.listdir(original_dataset_dir) if os.path.isdir(os.path.join(original_dataset_dir, d))]\n",
    "\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(original_dataset_dir, class_name)\n",
    "    images = os.listdir(class_dir)\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    train_images = images[:int(0.8 * len(images))]\n",
    "    test_images = images[int(0.8 * len(images)):]\n",
    "    \n",
    "    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
    "    \n",
    "    for image in train_images:\n",
    "        src = os.path.join(class_dir, image)\n",
    "        dst = os.path.join(train_dir, class_name, image)\n",
    "        shutil.copyfile(src, dst)\n",
    "    \n",
    "    for image in test_images:\n",
    "        src = os.path.join(class_dir, image)\n",
    "        dst = os.path.join(test_dir, class_name, image)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "print('Dữ liệu đã được chia thành công!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8800 images belonging to 6 classes.\n",
      "Found 2203 images belonging to 6 classes.\n",
      "Total training samples: 8800\n",
      "Total validation samples: 2203\n",
      "Training samples per label:\n",
      "Angry: 1502\n",
      "Disgusted: 1502\n",
      "Fearful: 1502\n",
      "Happy: 1502\n",
      "Neutral: 1290\n",
      "Sad: 1502\n",
      "Validation samples per label:\n",
      "Angry: 376\n",
      "Disgusted: 376\n",
      "Fearful: 376\n",
      "Happy: 376\n",
      "Neutral: 323\n",
      "Sad: 376\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\data_analysis\\speech_emotion_recognition\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 686ms/step - accuracy: 0.2603 - loss: 7.1443 - val_accuracy: 0.1723 - val_loss: 10.8694\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.2593 - val_loss: 9.4059\n",
      "Epoch 3/20\n",
      "\u001b[1m222/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 364ms/step - accuracy: 0.3941 - loss: 5.2867"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import Counter\n",
    "\n",
    "# Xây dựng mô hình CNN cải tiến\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Tạo Data Generator mà không sử dụng validation_split\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'D:/data_analysis/speech_emotion_recognition/data/EnglishDataset/split_data/train',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    'D:/data_analysis/speech_emotion_recognition/data/EnglishDataset/split_data/test',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Lấy số lớp từ train_generator\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "# Tạo và huấn luyện mô hình\n",
    "input_shape = (128, 128, 3)\n",
    "model = create_model(input_shape, num_classes)\n",
    "\n",
    "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
    "validation_steps = validation_generator.samples // validation_generator.batch_size\n",
    "\n",
    "print(f'Total training samples: {train_generator.samples}')\n",
    "print(f'Total validation samples: {validation_generator.samples}')\n",
    "\n",
    "# Đếm số lượng mẫu cho từng nhãn\n",
    "train_labels_count = Counter(train_generator.labels)\n",
    "validation_labels_count = Counter(validation_generator.labels)\n",
    "\n",
    "print(\"Training samples per label:\")\n",
    "for label, count in train_labels_count.items():\n",
    "    label_name = list(train_generator.class_indices.keys())[list(train_generator.class_indices.values()).index(label)]\n",
    "    print(f\"{label_name}: {count}\")\n",
    "\n",
    "print(\"Validation samples per label:\")\n",
    "for label, count in validation_labels_count.items():\n",
    "    label_name = list(validation_generator.class_indices.keys())[list(validation_generator.class_indices.values()).index(label)]\n",
    "    print(f\"{label_name}: {count}\")\n",
    "\n",
    "# Huấn luyện mô hình với early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Lưu mô hình đã huấn luyện\n",
    "model.save('emotion_recognition_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
